<div align="center">

<img src='https://user-images.githubusercontent.com/4397546/229094115-862c747e-7397-4b54-ba4a-bd368bfe2e0f.png' width='500px'/>


<!--<h2> ğŸ˜­ SadTalkerï¼š <span style="font-size:12px">Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation </span> </h2> -->

  <a href='https://arxiv.org/abs/2211.12194'><img src='https://img.shields.io/badge/ArXiv-PDF-red'></a> &nbsp; 


![sadtalker](https://user-images.githubusercontent.com/4397546/222490039-b1f6156b-bf00-405b-9fda-0c9a9156f991.gif)

<b>TL;DR: &nbsp;&nbsp;&nbsp;&nbsp; single image ğŸ™â€â™‚ï¸  &nbsp;&nbsp;&nbsp;&nbsp;+  &nbsp;&nbsp;&nbsp;&nbsp; audio ğŸ¤  &nbsp;&nbsp;&nbsp;&nbsp; =  &nbsp;&nbsp;&nbsp;&nbsp; talking head video ğŸ.</b>

<br>

</div>

## ğŸ“ 0. Paper

### Summary

ExpNet(ì–¼êµ´) + 

### Main Pipeline
![main pipeline](docs\main_pipeline.PNG)


1. image inputì„ coefficients of 3DMM(3D Morphable Model)ë¡œ ì²« ì´ë¯¸ì§€ì˜ facial expression($\beta_{0}$), head pose($\rho_{0}$)ë¥¼ ìƒì„±í•œë‹¤.
2. audio feature input($\alpha_{\{1...n\}}$)ì™€ $\beta_{0}$ë¥¼ ExpNetì— í†µê³¼ì‹œì¼œ ëˆˆê¹œë¹¡ì„, ì…ìˆ ëª¨ì–‘, ì–¼êµ´ í‘œì • ê°œì„ ì— ê´€í•œ ì—°ì†ê°’ì„ ì¶”ì¶œí•œë‹¤.($\beta_{\{1...n\}}$)
3. $\alpha_{\{1...n\}}$ì™€ $\rho_{0}$ë¥¼ PoseVAEì— í†µê³¼ì‹œì¼œ Styleì„ ì…íŒ ì—°ì†ê°’ì„ ì¶”ì¶œí•œë‹¤.($\rho_{\{1...n\}}$)
4. $\beta_{\{0...n\}}$ì™€ $\rho_{\{0...n\}}$ì„ 3D-Aware Face Render ëª¨ë“ˆì„ í†µí•´ ì—°ì†ì ì¸ Frameì„ ìƒì„±í•œë‹¤.


## âš™ï¸ 1. Installation

### Windows:

1. [Python 3.10.6](https://www.python.org/downloads/windows/) ì„¤ì¹˜, python í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì²´í¬í•˜ê¸°.
2. [git](https://git-scm.com/download/win) ì„¤ì¹˜
3. `ffmpeg` ì„¤ì¹˜, [ì—¬ê¸°](https://www.wikihow.com/Install-FFmpeg-on-Windows)ë¥¼ í†µí•´ ì„¤ì¹˜í•  ê²ƒ. (pip install ffmpeg ì•ˆë  ìˆ˜ ìˆìŒ).
4. commandì°½ì— `git clone https://github.com/Winfredy/SadTalker.git` ì…ë ¥.
5. [ì—¬ê¸°ì„œ](#ğŸ“¥-2-download-trained-models) `checkpoint`ë‘ `gfpgan`  ë‹¤ìš´ë¡œë“œ.

## ğŸ“¥ 2. Download Trained Models

**Google Driver**: [main checkpoints](https://drive.google.com/file/d/1gwWh45pF7aelNP_P78uDJL8Sycep-K7j/view?usp=sharing), [gfpgan](https://drive.google.com/file/d/19AIBsmfcHW6BRJmeqSFlG5fL445Xmsyi?usp=sharing)




Model explains:

| Model | Description
| :--- | :----------
|checkpoints/mapping_00229-model.pth.tar | Pre-trained MappingNet in Sadtalker.
|checkpoints/mapping_00109-model.pth.tar | Pre-trained MappingNet in Sadtalker.
|checkpoints/SadTalker_V0.0.2_256.safetensors | packaged sadtalker checkpoints of old version, 256 face render.
|checkpoints/SadTalker_V0.0.2_512.safetensors | packaged sadtalker checkpoints of old version, 512 face render.
|gfpgan/weights | Face detection and enhanced models used in `facexlib` and `gfpgan`.


## ğŸ”® 3. Quick Start

1. conda ê°€ìƒí™˜ê²½ ì„¤ì¹˜
2. git clone
3. requirements ì„¤ì¹˜( 230628 ê¸°ì¤€ requirements)
4. torch cuda11.7 ì„¤ì¹˜( 3070 Ti ê¸°ì¤€)

```command
conda create -n sad-talker python=3.10
git clone https://github.com/Winfredy/SadTalker.git
pip install -r requirements.txt
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
```
5. Inference

```bash
# driven_audio   : ë§Œë“¤ ì˜ìƒì˜ input wav      (default : ./examples/driven_audio/bus_chinese.wav)
# source_image   : ë§Œë“¤ ì˜ìƒì˜ image or video (default : ./examples/source_image/full_body_1.png)
# result_dir     : output í´ë”               (default : ./results)
# still          : ì–¼êµ´ë§Œ cropí•˜ëŠ” ì˜µì…˜
# enhancer       : ì–¼êµ´ ë” ìì—°ìŠ¤ëŸ½ê²Œ í›„ì²˜ë¦¬   (default : None)
# checkpoint_dir : ëª¨ë¸ checkpoint í´ë”       (default : ./checkpoints) 
# size           : facerender size           (default : 256)
```
ë‹¤ë¥¸ ì˜µì…˜ë“¤ì€ inference.pyì— ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```bash
python inference.py --driven_audio 0_0_00004.wav \ 
--source_image UJaeSuck1.mp4 \
--result_dir ./results \
--still \
--enhancer gfpgan \
--checkpoint_dir SadTalker \
--size 512

```

## ğŸ› Citation

If you find our work useful in your research, please consider citing:

```bibtex
@article{zhang2022sadtalker,
  title={SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation},
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  journal={arXiv preprint arXiv:2211.12194},
  year={2022}
}
```


